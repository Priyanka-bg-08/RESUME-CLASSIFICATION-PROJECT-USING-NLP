{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b15bb0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scipy in c:\\users\\priya\\anaconda3\\lib\\site-packages (1.7.3)\n",
      "Requirement already satisfied: numpy<1.23.0,>=1.16.5 in c:\\users\\priya\\anaconda3\\lib\\site-packages (from scipy) (1.19.5)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install scipy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b5e2584",
   "metadata": {},
   "source": [
    "## 1. Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0960fca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import nltk\n",
    "import spacy\n",
    "import string\n",
    "import textract\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from matplotlib import pylab\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "46cb89af",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['PeopleSoft', 'React JS Developer', 'SQL Developer', 'Workday']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir('Resumes/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7a4d6f3",
   "metadata": {},
   "source": [
    "## 2. Exctract the Text from Dataset"
   ]
  },
  {
   "cell_type": "raw",
   "id": "35c6bfc7",
   "metadata": {},
   "source": [
    "# try to extract all folder one time\n",
    "import os\n",
    "\n",
    "file_name = []\n",
    "category = []\n",
    "base_dir = './Resume_Docx'\n",
    "for root, dirs, files in os.walk(base_dir, topdown=False):\n",
    "    for name in files:\n",
    "        if name.endswith(\".docx\" ):\n",
    "            file_path = os.path.join(root, name)\n",
    "            file_name.append(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8cde072b",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] The system cannot find the path specified: 'Resume_Docx/PeopleSoft/'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [4]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m category1  \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m      3\u001b[0m directory1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mResume_Docx/PeopleSoft/\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdirectory1\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.docx\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m      6\u001b[0m         os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(directory1, i)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] The system cannot find the path specified: 'Resume_Docx/PeopleSoft/'"
     ]
    }
   ],
   "source": [
    "file_path1 = []\n",
    "category1  = []\n",
    "directory1 = 'Resume_Docx/PeopleSoft/'\n",
    "for i in os.listdir(directory1):\n",
    "    if i.endswith('.docx'):\n",
    "        os.path.join(directory1, i)\n",
    "        file_path1.append((textract.process(os.path.join(directory1, i))).decode('utf-8'))\n",
    "        category1.append('PeopleSoft')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49f37bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = pd.DataFrame(data = file_path1 , columns = ['Raw_Details'])\n",
    "data1['Category1'] = category1\n",
    "data1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03a3912f",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path2 = []\n",
    "category2  = []\n",
    "directory2 = 'Resume_Docx/React JS Developer/'\n",
    "for i in os.listdir(directory2):\n",
    "    if i.endswith('.docx'):\n",
    "        os.path.join(directory2, i)\n",
    "        file_path2.append((textract.process(os.path.join(directory2, i))).decode('utf-8'))\n",
    "        category2.append('React JS Developer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "058b86ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = pd.DataFrame(data = file_path2 , columns = ['Raw_Details'])\n",
    "data2['Category2'] = category2\n",
    "data2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53bef451",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path3 = []\n",
    "category3  = []\n",
    "directory3 = 'Resume_Docx/SQL Developer/'\n",
    "for i in os.listdir(directory3):\n",
    "    if i.endswith('.docx'):\n",
    "        os.path.join(directory3, i)\n",
    "        file_path3.append((textract.process(os.path.join(directory3, i))).decode('utf-8'))\n",
    "        category3.append('SQL Developer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52bdcae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data3 = pd.DataFrame(data = file_path3 , columns = ['Raw_Details'])\n",
    "data3['Category3'] = category3\n",
    "data3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8907d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path4 = []\n",
    "category4  = []\n",
    "directory4 = 'Resume_Docx/Workday/'\n",
    "for i in os.listdir(directory4):\n",
    "    if i.endswith('.docx'):\n",
    "        os.path.join(directory4, i)\n",
    "        file_path4.append((textract.process(os.path.join(directory4, i))).decode('utf-8'))\n",
    "        category4.append('Workday')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "333251ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "data4 = pd.DataFrame(data = file_path4 , columns = ['Raw_Details'])\n",
    "data4['Category4'] = category4\n",
    "data4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37783975",
   "metadata": {},
   "source": [
    "## 3. Create a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d48e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "resume_data = data1.append([data2, data3, data4], ignore_index = True)\n",
    "resume_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a7104b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "resume_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eebe8e0e",
   "metadata": {},
   "source": [
    "### 3.1 Merge all Unnecessary column in One Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f24e9812",
   "metadata": {},
   "outputs": [],
   "source": [
    "resume_data['Category'] = category1 + category2 + category3 + category4\n",
    "resume_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbcc5b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "resume_data.drop(['Category1', 'Category2', 'Category3', 'Category4'], axis = 1, inplace = True)\n",
    "resume_data = resume_data[[\"Category\", \"Raw_Details\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58eb57a0",
   "metadata": {},
   "source": [
    "### 3.2 Final Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49822ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "resume_data.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a44664",
   "metadata": {},
   "outputs": [],
   "source": [
    "resume_data[\"Raw_Details\"][4]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74ed94b1",
   "metadata": {},
   "source": [
    "### 3.3 Save new CSV File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd96ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "resume_data.to_csv('Raw_Resume.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca800f49",
   "metadata": {},
   "source": [
    "## 4. Data Understanding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53bd8e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "resume_data = pd.read_csv(\"Raw_Resume.csv\")\n",
    "resume_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9948b5f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "resume_data[resume_data.Category == 'Workday']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35837d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "resume_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6e6b950",
   "metadata": {},
   "source": [
    "### 4.1 Number of Words in each Resume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f40e4de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "resume_data['Word_Count'] = resume_data['Raw_Details'].apply(lambda x: len(str(x).split(\" \")))\n",
    "resume_data[['Raw_Details','Word_Count']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80606b74",
   "metadata": {},
   "source": [
    "### 4.2 Number of Characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50470883",
   "metadata": {},
   "outputs": [],
   "source": [
    "resume_data['Char_Count'] = resume_data['Raw_Details'].str.len() ## this also includes spaces\n",
    "resume_data[['Raw_Details','Char_Count']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "783b2a24",
   "metadata": {},
   "source": [
    "### 4.3 Number of Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5de11fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "resume_data['Stopwords'] = resume_data['Raw_Details'].apply(lambda x: len([x for x in x.split() if x in stop]))\n",
    "resume_data[['Raw_Details','Stopwords']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f79ae3",
   "metadata": {},
   "source": [
    "### 4.4 Number of Numerics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e9c0b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "resume_data['Numerics'] = resume_data['Raw_Details'].apply(lambda x: len([x for x in x.split() if x.isdigit()]))\n",
    "resume_data[['Raw_Details','Numerics']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df0c87f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "resume_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84c217fa",
   "metadata": {},
   "source": [
    "## 5. Text Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc01302",
   "metadata": {},
   "source": [
    "### 5.1 Using Regular Expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae274eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(sentence):\n",
    "    sentence = str(sentence)\n",
    "    sentence = sentence.lower()\n",
    "    sentence = sentence.replace('{html}',\"\") \n",
    "    cleanr = re.compile('<.*?>')\n",
    "    cleantext = re.sub(cleanr, '', sentence)\n",
    "    rem_url = re.sub(r'http\\S+', '',cleantext)\n",
    "    rem_num = re.sub('[0-9]+', '', rem_url)\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    tokens = tokenizer.tokenize(rem_num)  \n",
    "    filtered_words = [w for w in tokens if len(w) > 2 if not w in stopwords.words('english')]\n",
    "\n",
    "    return \" \".join(filtered_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d08ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "resume_data = pd.read_csv('Raw_Resume.csv')\n",
    "resume_data['Resume_Details'] = resume_data.Raw_Details.apply(lambda x: preprocess(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99c80ebb",
   "metadata": {},
   "source": [
    "### 5.2 Clean Text from Resume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe7f207",
   "metadata": {},
   "outputs": [],
   "source": [
    "resume_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad06c41e",
   "metadata": {},
   "source": [
    "### 5.3 Save the Clean Data in new CSV File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45fd7ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "resume_data.drop(['Raw_Details'], axis = 1, inplace = True)\n",
    "resume_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "963fbfef",
   "metadata": {},
   "outputs": [],
   "source": [
    "resume_data.to_csv('Cleaned_Resumes.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f02bb314",
   "metadata": {},
   "outputs": [],
   "source": [
    "resume_data = pd.read_csv('Cleaned_Resumes.csv')\n",
    "resume_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9924d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "resume_data.Resume_Details[5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e1d7cf1",
   "metadata": {},
   "source": [
    "## 6. Named Entity Recognition (NER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da5821e",
   "metadata": {},
   "outputs": [],
   "source": [
    "oneSetOfStopWords = set(stopwords.words('english')+['``',\"''\"])\n",
    "totalWords =[]\n",
    "Sentences = resume_data['Resume_Details'].values\n",
    "cleanedSentences = \"\"\n",
    "for records in Sentences:\n",
    "    cleanedText = preprocess(records)\n",
    "    cleanedSentences += cleanedText\n",
    "    requiredWords = nltk.word_tokenize(cleanedText)\n",
    "    for word in requiredWords:\n",
    "        if word not in oneSetOfStopWords and word not in string.punctuation:\n",
    "            totalWords.append(word)\n",
    "    \n",
    "wordfreqdist = nltk.FreqDist(totalWords)\n",
    "mostcommon = wordfreqdist.most_common(50)\n",
    "print(mostcommon)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e00d230",
   "metadata": {},
   "source": [
    "## 7. Parts Of Speech (POS) Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3123a04d",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_lg')\n",
    "\n",
    "one_block = cleanedSentences[1300:5200]\n",
    "doc_block = nlp(one_block)\n",
    "spacy.displacy.render(doc_block, style= 'ent', jupyter= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31dc8951",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for token in doc_block[:30]:\n",
    "    print(token,token.pos_)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7b9084d",
   "metadata": {},
   "source": [
    "### 7.1 Filtering out only the Nouns and Verbs from the Text to Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad9e5df",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_block = cleanedSentences\n",
    "doc_block = nlp(one_block)\n",
    "nouns_verbs = [token.text for token in doc_block if token.pos_ in ('NOUN','VERB')]\n",
    "print(nouns_verbs[:250])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01c5959a",
   "metadata": {},
   "source": [
    "### 7.2 Counting all the Nouns and Verbs present in the Tokens of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af9ffdcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer()\n",
    "X = cv.fit_transform(nouns_verbs)\n",
    "sum_words = X.sum(axis=0)\n",
    "\n",
    "words_freq = [(word,sum_words[0,idx]) for word, idx in cv.vocabulary_.items()]\n",
    "words_freq = sorted(words_freq, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "wd_df = pd.DataFrame(words_freq)\n",
    "wd_df.columns = ['Words','Count']\n",
    "wd_df[0:15]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a885938",
   "metadata": {},
   "source": [
    "### 7.3 Visualizing the Result of Top 20 Nouns and Verbs most Frequently"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "278402c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axe = plt.subplots(1,1, figsize=(10,7), dpi=200)\n",
    "ax = sns.barplot(x= wd_df['Count'].head(20), y= wd_df.Words.head(20), data= wd_df, ax = axe,\n",
    "            label= 'Total Pofile Category : {}'.format(len(resume_data.Category.unique())))\n",
    "\n",
    "axe.set_xlabel('Frequency', size=16,fontweight= 'bold')\n",
    "axe.set_ylabel('Words', size=16, fontweight= 'bold')\n",
    "plt.xticks(rotation = 0)\n",
    "plt.legend(loc='best', fontsize= 'x-large')\n",
    "plt.title('Top 25 Most used Nouns and Verbs in Resumes', fontsize= 18, fontweight= 'bold')\n",
    "rcParams = {'xtick.labelsize':'14','ytick.labelsize':'14','axes.labelsize':'16'}\n",
    "\n",
    "for i in ax.containers:\n",
    "    ax.bar_label(i,color = 'black', fontweight = 'bold', fontsize= 12)\n",
    "\n",
    "pylab.rcParams.update(rcParams)\n",
    "fig.tight_layout()\n",
    "plt.show()\n",
    "fig.savefig('IMG/Top_Nouns_Verbs_Bar', dpi = 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de32859d",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \" \".join(cat for cat in wd_df.Words) # Creating the text variable\n",
    "\n",
    "word_cloud = WordCloud(width=1000, height=800, random_state=10, background_color=\"black\", \n",
    "                       colormap=\"Pastel1\", collocations=False, stopwords=STOPWORDS).generate(text)\n",
    "\n",
    "plt.figure(figsize=(10,7), dpi=800) # Display the generated Word Cloud\n",
    "plt.title('Most used Nouns and Verbs in Resumes', fontsize= 15, fontweight= 'bold')\n",
    "plt.imshow(word_cloud)\n",
    "plt.axis(\"off\")\n",
    "\n",
    "word_cloud.to_file('IMG/Word_Clowds_Noun_Verb.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28c4acd8",
   "metadata": {},
   "source": [
    "# THE END !!!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
